{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Keras\n",
    "import keras\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model=keras.models.load_model(\"hate&abusive_model.h5\")\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    load_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A96 A90 Aberdeen - A92 Aberdeen - Closure, All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G'night, that's me signing off. We'll be back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1 - Tyne Bridge - Weather, All lanes restrict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clackmannanshire - Weather, Drivers in Clackma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>West Lothian - Weather, Drivers in West Lothia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  A96 A90 Aberdeen - A92 Aberdeen - Closure, All...\n",
       "1  G'night, that's me signing off. We'll be back ...\n",
       "2  A1 - Tyne Bridge - Weather, All lanes restrict...\n",
       "3  Clackmannanshire - Weather, Drivers in Clackma...\n",
       "4  West Lothian - Weather, Drivers in West Lothia..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv('tweets.csv', header=None, names=['tweet'])\n",
    "#test.rename(columns ={'tweet'}, inplace = True)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'but', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you','youu','yous', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "STOPWORDS = set(stopwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aberdeen, , aberdeen, , closur, lane, close, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[gnight, that, sign, off, well, back, , tomorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[, tyne, bridg, , weather, lane, restrict, dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[clackmannanshir, , weather, driver, clackmann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[west, lothian, , weather, driver, west, lothi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[falkirk, , weather, driver, falkirk, advis, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[stirl, , weather, driver, stirl, advis, use, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[got, somewher, need, unsur, road, are\\r\\n\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[south, lanarkshir, , weather, driver, south, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[north, lanarkshir, , weather, driver, north, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  [aberdeen, , aberdeen, , closur, lane, close, ...\n",
       "1  [gnight, that, sign, off, well, back, , tomorr...\n",
       "2  [, tyne, bridg, , weather, lane, restrict, dir...\n",
       "3  [clackmannanshir, , weather, driver, clackmann...\n",
       "4  [west, lothian, , weather, driver, west, lothi...\n",
       "5  [falkirk, , weather, driver, falkirk, advis, u...\n",
       "6  [stirl, , weather, driver, stirl, advis, use, ...\n",
       "7  [got, somewher, need, unsur, road, are\\r\\n\\r\\n...\n",
       "8  [south, lanarkshir, , weather, driver, south, ...\n",
       "9  [north, lanarkshir, , weather, driver, north, ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv('tweets.csv', header=None, names=['tweet'])\n",
    "#test.rename(columns ={'tweet'}, inplace = True)\n",
    "test.head(5)\n",
    "\n",
    "test0 = test1 = test\n",
    "\n",
    "english_punctuations = string.punctuation\n",
    "st = nltk.PorterStemmer()\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'w+')\n",
    "\n",
    "def cleaning_data(data):\n",
    "    \n",
    "    data=data.lower().strip()\n",
    "    #Removing everything other than a-z\n",
    "    data = re.sub('[^a-z\\\\s]+', '', data)\n",
    "         \n",
    "    #Removing punctuations\n",
    "    translator = str.maketrans('', '', english_punctuations)\n",
    "    data = data.translate(translator)\n",
    "    \n",
    "    #Removing repeating characters\n",
    "    data = re.sub(r'(.)1+', r'1', data)\n",
    "    \n",
    "    #Removing URLs\n",
    "    data = re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "    \n",
    "    #Removing stopwords and tokenizing\n",
    "    d=[]\n",
    "    words=data.split(\" \")\n",
    "    for w in words:\n",
    "        if w not in STOPWORDS:\n",
    "            d.append(w)\n",
    "    data=d\n",
    "    \n",
    "    #Stemming and Lemmatize\n",
    "    data = [st.stem(word) for word in data]\n",
    "    data = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "\n",
    "test0['tweet'] = test['tweet'].apply(lambda text: cleaning_data(text))\n",
    "test0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq [[16391, 16391, 4979, 824, 2138], [14, 1305, 114, 191, 109, 262], [990, 4979, 723], [990, 2996, 376, 22017, 1718, 691, 6689, 937], [2145, 990, 2996, 2145, 376, 22017, 1718, 691, 6689, 937], [990, 2996, 376, 22017, 1718, 691, 6689, 937], [990, 2996, 376, 22017, 1718, 691, 6689, 937], [56, 106, 1465, 250, 3022, 2702, 1362, 717, 44, 3088, 34208, 8114], [1510, 990, 2996, 1510, 376, 22017, 1718, 691, 6689, 937], [5040, 990, 2996, 5040, 376, 22017, 1718, 691, 6689, 937], [187, 284, 106, 422, 1423, 223, 7058, 187, 101, 1576, 274, 187], [8, 23505, 1182, 824, 647, 5040, 623, 1845, 677, 2500, 1266], [1142, 4945, 630, 173, 1372, 4595, 2500, 87, 173, 80, 949, 465, 1142, 842, 1129], [16391, 16391, 4979, 824], [308, 4979, 723, 2138], [34208, 105, 101, 2492], [791, 291, 857, 4979, 824], [174, 78, 2664, 106, 18924, 36809, 223, 2036, 78, 2025, 2222, 9957, 422, 574], [14, 1498, 12809, 211, 198, 417], [4979, 824, 38950, 2138], [791, 1465, 4979, 824], [650, 20, 376, 436, 5606, 388, 937, 15349, 100, 436], [3885, 781, 4979, 824, 38950], [4979, 824, 38950], [4979, 791, 4979, 824], [990, 4979, 723, 2138], [86, 56, 518, 1040, 1395, 26018], [990, 4979, 723], [5159, 376, 2664, 422, 574, 2025, 5159, 2411, 11402, 491, 2645], [457, 2687, 211, 10411, 750], [8, 7698, 16406, 3529], [120, 9718, 1465, 3025, 1465, 13964, 8573, 9718, 1465], [4979, 824, 38950], [27372, 491], [1800, 824, 723, 354, 2333, 154, 300, 1305, 2225, 447], [24762, 366, 3213, 990, 24762, 824, 366, 322, 1718, 366, 3213, 6689, 937], [5159, 491, 5159, 3404, 5159, 1604], [2664, 106, 2225, 36809, 2036, 2664, 2025, 2222, 9957, 422, 574], [91, 42, 2225, 1462, 44, 21, 100, 18924, 154, 46089, 154, 526, 44], [3407], [8, 4385, 48, 10559, 5297, 3278, 5238], [326, 138, 5855, 485, 198, 191, 109, 262, 106, 422, 113, 428, 447], [8, 101, 201, 1719, 11800, 203, 78, 422, 43, 42229, 78, 1462, 82, 159, 2441, 35], [8, 5297, 16945, 542, 1031, 223], [623, 376, 31864, 1604, 2193, 717, 1604, 16391, 341, 4255, 48, 458, 16391], [126, 106, 422, 2230, 2344, 14914, 44, 777, 504], [464, 403], [], [630, 1142, 126, 2417, 1374, 357, 121, 28208], [938, 518, 87, 109, 59], [534, 569, 717], [7], [2582, 3620, 6911, 3250, 1543], [8, 198, 990, 15012], [8, 1465], [157, 13426, 114, 223, 1601, 2492, 82, 159, 81], [129]]\n",
      "pred [[1.27827436e-01]\n",
      " [3.34489644e-02]\n",
      " [1.89208984e-03]\n",
      " [8.68852735e-02]\n",
      " [3.96490097e-03]\n",
      " [8.68852735e-02]\n",
      " [8.68852735e-02]\n",
      " [1.52498484e-04]\n",
      " [2.99646199e-01]\n",
      " [1.29474431e-01]\n",
      " [4.41187739e-01]\n",
      " [7.43514299e-02]\n",
      " [1.00628138e-02]\n",
      " [5.58836281e-01]\n",
      " [2.16454268e-04]\n",
      " [4.51743603e-04]\n",
      " [2.09653258e-01]\n",
      " [1.10603571e-02]\n",
      " [2.91401148e-03]\n",
      " [1.53123140e-02]\n",
      " [4.25249934e-02]\n",
      " [2.31025219e-02]\n",
      " [1.43718630e-01]\n",
      " [1.15765184e-01]\n",
      " [1.31089061e-01]\n",
      " [3.38226557e-04]\n",
      " [4.12049890e-03]\n",
      " [1.89208984e-03]\n",
      " [8.08241963e-03]\n",
      " [9.74647403e-02]\n",
      " [1.22643709e-02]\n",
      " [3.92654538e-03]\n",
      " [1.15765184e-01]\n",
      " [5.27441978e-01]\n",
      " [3.22312117e-04]\n",
      " [6.13714218e-01]\n",
      " [8.11424971e-01]\n",
      " [8.02710652e-03]\n",
      " [7.00056553e-04]\n",
      " [1.29357040e-01]\n",
      " [1.27246827e-01]\n",
      " [7.78287649e-04]\n",
      " [1.90994143e-03]\n",
      " [9.81685162e-01]\n",
      " [6.95466995e-04]\n",
      " [2.15183198e-02]\n",
      " [5.33524394e-01]\n",
      " [2.43931115e-01]\n",
      " [2.54016817e-02]\n",
      " [4.49375302e-01]\n",
      " [3.20649445e-02]\n",
      " [9.98351812e-01]\n",
      " [7.39949048e-01]\n",
      " [1.09274983e-02]\n",
      " [6.17018044e-02]\n",
      " [1.81217790e-02]\n",
      " [3.68898451e-01]]\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "hate and abusive\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "hate and abusive\n",
      "no hate\n",
      "hate and abusive\n",
      "hate and abusive\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "hate and abusive\n",
      "no hate\n",
      "no hate\n",
      "hate and abusive\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "hate and abusive\n",
      "hate and abusive\n",
      "no hate\n",
      "no hate\n",
      "no hate\n",
      "no hate\n"
     ]
    }
   ],
   "source": [
    "seq = load_tokenizer.texts_to_sequences(test0['tweet'])\n",
    "padded = sequence.pad_sequences(seq, maxlen=300)\n",
    "print(\"seq\",seq)\n",
    "pred =[]\n",
    "hate=0\n",
    "no_hate=0\n",
    "pred = load_model.predict(padded)\n",
    "print(\"pred\", pred)\n",
    "for p in pred:\n",
    "    if p<0.5:\n",
    "        no_hate+=1\n",
    "        print(\"no hate\")\n",
    "    else:\n",
    "        hate+=1\n",
    "        print(\"hate and abusive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of hate tweets: 8\n",
      "Total number of non-hate tweets: 49\n",
      "Hate & Abusive : 14.035087719298245 %\n",
      "No Hate : 85.96491228070175 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of hate tweets:\",hate)\n",
    "print(\"Total number of non-hate tweets:\",no_hate)\n",
    "print(\"Hate & Abusive :\",(hate/(hate+no_hate))*100,\"%\")\n",
    "print(\"No Hate :\",(no_hate/(hate+no_hate))*100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
